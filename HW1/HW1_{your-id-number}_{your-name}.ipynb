{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a5a31d058a6316b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 作业一：实现HMM中文分词和BPE英文分词\n",
    "姓名：\n",
    "\n",
    "学号："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7a16ed",
   "metadata": {},
   "source": [
    "## 任务一：HMM模型用于中文分词\n",
    "\n",
    "任务一评分标准：\n",
    "1. 共有8处TODO需要填写，每个TODO计1-2分，共9分，预计代码量30行。\n",
    "2. 允许自行修改、编写代码完成。对于该情况，请补充注释以便于评分，否则结果不正确将导致较多的扣分。\n",
    "3. 用于说明实验的文字和总结不额外计分，但不写会导致扣分。\n",
    "\n",
    "注：本任务仅在短句子上进行效果测试，因此对概率的计算可直接进行连乘。在实践中，常先对概率取对数，将连乘变为加法来计算，以避免出现数值溢出的情况。\n",
    "\n",
    "> 你可以像这样在Markdown单元格中使用引用符号`>`，  \n",
    "以及在代码单元格中使用注释来说明你的实验。  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5aba6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f7d77db9",
   "metadata": {},
   "source": [
    "导入HMM参数，初始化所需的起始概率矩阵、转移概率矩阵、发射概率矩阵。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d25beba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "with open(\"hmm_parameters.pkl\", \"rb\") as f:\n",
    "    hmm_parameters = pickle.load(f)\n",
    "\n",
    "# 非断字（B）为第0行，断字（I）为第1行\n",
    "# 发射概率矩阵中，词典大小为65536，以汉字的Unicode码点（一个整数值）作为行索引\n",
    "start_probability = hmm_parameters[\"start_prob\"]  # shape(2,)\n",
    "trans_matrix = hmm_parameters[\"trans_mat\"]  # shape(2, 2)\n",
    "emission_matrix = hmm_parameters[\"emission_mat\"]  # shape(2, 65536)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7070152",
   "metadata": {},
   "source": [
    "定义待处理的句子："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87219e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: 将your_name中的xxx替换为你的姓名【1分】\n",
    "your_name = \"胡文灿\"\n",
    "input_sentence = f\"{your_name}是一名优秀的学生\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1035cbc7",
   "metadata": {},
   "source": [
    "实现Viterbi算法，并以此进行中文分词。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1adac849",
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi(sent_orig: str, start_prob: np.ndarray, trans_mat: np.ndarray, emission_mat: np.ndarray) -> str:\n",
    "    \"\"\"\n",
    "    Viterbi算法进行中文分词。\n",
    "\n",
    "    Args:\n",
    "        sent_orig: str - 输入的句子\n",
    "        start_prob: numpy.ndarray - 起始概率矩阵\n",
    "        trans_mat: numpy.ndarray - 转移概率矩阵\n",
    "        emission_mat: numpy.ndarray - 发射概率矩阵\n",
    "\n",
    "    Return:\n",
    "        str - 中文分词的结果\n",
    "    \"\"\"\n",
    "\n",
    "    #  将汉字转为数字表示\n",
    "    sent_ord = [ord(x) for x in sent_orig]\n",
    "\n",
    "    # `dp`用来储存不同位置每种标注（B/I）的最大概率值\n",
    "    dp = np.zeros((2, len(sent_ord)), dtype=float)\n",
    "\n",
    "    # `path`用来储存最大概率对应的上步B/I选择\n",
    "    #  例如 path[1][7] == 1 意味着第8个（从1开始计数）字符标注I对应的最大概率，其前一步的隐状态为1（I）\n",
    "    #  例如 path[0][5] == 1 意味着第6个字符标注B对应的最大概率，其前一步的隐状态为1（I）\n",
    "    #  例如 path[1][1] == 0 意味着第2个字符标注I对应的最大概率，其前一步的隐状态为0（B）\n",
    "    path = np.zeros((2, len(sent_ord)), dtype=int)\n",
    "\n",
    "    #  TODO: 第一个位置的最大概率值计算【1分】\n",
    "    dp[0, 0] = start_prob[0] + emission_mat[0, sent_ord[0]]\n",
    "    dp[1, 0] = start_prob[1] + emission_mat[1, sent_ord[0]]\n",
    "\n",
    "    #  TODO: 其余位置的最大概率值计算（填充dp和path矩阵）【2分】\n",
    "    for i in range(1, len(sent_ord)):\n",
    "        for j in range(2):\n",
    "            max_prob = -float('inf')\n",
    "            max_path = 0\n",
    "            for k in range(2):\n",
    "                prob = dp[k, i-1] + trans_mat[k, j] + emission_mat[j, sent_ord[i]]\n",
    "                if prob > max_prob:\n",
    "                    max_prob = prob\n",
    "                    max_path = k\n",
    "            dp[j, i] = max_prob\n",
    "            path[j, i] = max_path\n",
    "\n",
    "    #  `labels`用来储存每个位置最有可能的隐状态\n",
    "    labels = [0 for _ in range(len(sent_ord))]\n",
    "\n",
    "    #  TODO: 计算labels每个位置上的值（填充labels矩阵）【1分】\n",
    "    max_prob = -float('inf')\n",
    "    max_path = 0\n",
    "    for j in range(2):\n",
    "        if dp[j, -1] > max_prob:\n",
    "            max_prob = dp[j, -1]\n",
    "            max_path = j\n",
    "    labels[-1] = max_path\n",
    "    for i in range(len(sent_ord) - 2, -1, -1):\n",
    "        labels[i] = path[labels[i+1], i+1]\n",
    "\n",
    "    #  根据lalels生成切分好的字符串\n",
    "    sent_split = []\n",
    "    for idx, label in enumerate(labels):\n",
    "        if label == 1:\n",
    "            sent_split += [sent_ord[idx], ord(\"/\")]\n",
    "        else:\n",
    "            sent_split += [sent_ord[idx]]\n",
    "    sent_split_str = \"\".join([chr(x) for x in sent_split])\n",
    "\n",
    "    return sent_split_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d795414b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Viterbi算法分词结果： 胡文/灿是/一名/优秀/的/学生/\n"
     ]
    }
   ],
   "source": [
    "print(\"Viterbi算法分词结果：\", viterbi(input_sentence, start_probability, trans_matrix, emission_matrix))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60fcafdb",
   "metadata": {},
   "source": [
    "实现前向算法，计算该句子的概率值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf6796a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_prob_by_forward(sent_orig: str, start_prob: np.ndarray, trans_mat: np.ndarray, emission_mat: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    前向算法，计算输入中文句子的概率值。\n",
    "\n",
    "    Args:\n",
    "        sent_orig: str - 输入的句子\n",
    "        start_prob: numpy.ndarray - 起始概率矩阵\n",
    "        trans_mat: numpy.ndarray - 转移概率矩阵\n",
    "        emission_mat: numpy.ndarray - 发射概率矩阵\n",
    "\n",
    "    Return:\n",
    "        float - 概率值\n",
    "    \"\"\"\n",
    "\n",
    "    #  将汉字转为数字表示\n",
    "    sent_ord = [ord(x) for x in sent_orig]\n",
    "\n",
    "    # `dp`用来储存不同位置每种隐状态（B/I）下，到该位置为止的句子的概率\n",
    "    dp = np.zeros((2, len(sent_ord)), dtype=float)\n",
    "\n",
    "    # TODO: 初始位置概率的计算【1分】\n",
    "    dp[0, 0] = start_prob[0] + emission_mat[0, sent_ord[0]]\n",
    "    dp[1, 0] = start_prob[1] + emission_mat[1, sent_ord[0]]\n",
    "\n",
    "    # TODO: 先计算其余位置的概率（填充dp矩阵），然后返回概率值【1分】\n",
    "    for i in range(1, len(sent_ord)):\n",
    "        for j in range(2):  # j=0 for B, j=1 for I\n",
    "            # 计算从状态k转移到状态j，并发射第i个字符的概率\n",
    "            for k in range(2):  # k=0 for B, k=1 for I\n",
    "                dp[j, i] += dp[k, i-1] * trans_mat[k, j] * emission_mat[j, sent_ord[i]]\n",
    "\n",
    "    return sum([dp[i][len(sent_ord) - 1] for i in range(2)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59533cd8",
   "metadata": {},
   "source": [
    "实现后向算法，计算该句子的概率值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e898306",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_prob_by_backward(sent_orig: str, start_prob: np.ndarray, trans_mat: np.ndarray, emission_mat: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    后向算法，计算输入中文句子的概率值。\n",
    "\n",
    "    Args:\n",
    "        sent_orig: str - 输入的句子\n",
    "        start_prob: numpy.ndarray - 起始概率矩阵\n",
    "        trans_mat: numpy.ndarray - 转移概率矩阵\n",
    "        emission_mat: numpy.ndarray - 发射概率矩阵\n",
    "\n",
    "    Return:\n",
    "        float - 概率值\n",
    "    \"\"\"\n",
    "\n",
    "    #  将汉字转为数字表示\n",
    "    sent_ord = [ord(x) for x in sent_orig]\n",
    "\n",
    "    # `dp`用来储存不同位置每种隐状态（B/I）下，从结尾到该位置为止的句子的概率\n",
    "    dp = np.zeros((2, len(sent_ord)), dtype=float)\n",
    "\n",
    "    # TODO: 终末位置概率的初始化【1分】\n",
    "    dp[:, -1] = emission_mat[:, sent_ord[-1]]\n",
    "\n",
    "    # TODO: 先计算其余位置的概率（填充dp矩阵），然后返回概率值【1分】\n",
    "    for i in range(len(sent_ord) - 2, -1, -1):\n",
    "        for j in range(2):  # j=0 for B, j=1 for I\n",
    "            dp[j, i] = np.sum(trans_mat[j, :] * dp[:, i + 1]) * emission_mat[j, sent_ord[i]]\n",
    "\n",
    "    return sum([dp[i][0] * start_prob[i] * emission_mat[i][sent_ord[0]] for i in range(2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b26101d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "前向算法概率： 7.04146073633844e-30\n",
      "后向算法概率： 4.039092824218213e-37\n"
     ]
    }
   ],
   "source": [
    "print(\"前向算法概率：\", compute_prob_by_forward(input_sentence, start_probability, trans_matrix, emission_matrix))\n",
    "print(\"后向算法概率：\", compute_prob_by_backward(input_sentence, start_probability, trans_matrix, emission_matrix))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20803f6a1a465dd6",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 实验总结\n",
    "> TODO：请在这里填写实验总结。\n",
    "主要是隐马尔可夫链的使用和理解动态规划"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e994be6e",
   "metadata": {},
   "source": [
    "## 任务二：BPE算法用于英文分词"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1cc4775",
   "metadata": {},
   "source": [
    "任务二评分标准：\n",
    "\n",
    "1. 共有6处TODO需要填写，每个TODO计1-2分，共9分，预计代码量50行。\n",
    "2. 允许自行修改、编写代码完成。对于该情况，请补充注释以便于评分，否则结果不正确将导致较多的扣分。\n",
    "3. 用于说明实验的文字和总结不额外计分，但不写会导致扣分。\n",
    "\n",
    "> 你可以像这样在Markdown单元格中使用引用符号`>`，  \n",
    "以及在代码单元格中使用注释来说明你的实验。  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f02463b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List, Tuple, Dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb5dbb9",
   "metadata": {},
   "source": [
    "构建空格分词器，将语料中的句子以空格切分成单词，然后将单词拆分成字母加`</w>`的形式。例如`apple`将变为`a p p l e </w>`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d6c3667a",
   "metadata": {},
   "outputs": [],
   "source": [
    "_splitor_pattern = re.compile(r\"[^a-zA-Z']+|(?=')\")\n",
    "_digit_pattern = re.compile(r\"\\d+\")\n",
    "\n",
    "\n",
    "def white_space_tokenize(corpus: List[str]) -> List[List[str]]:\n",
    "    \"\"\"\n",
    "    先正则化（字母转小写、数字转为N、除去标点符号），然后以空格分词语料中的句子，例如：  \n",
    "    输入 `corpus = [\"I am happy.\", \"I have 10 apples!\"]`，  \n",
    "    得到 `[[\"i\", \"am\", \"happy\"], [\"i\", \"have\", \"N\", \"apples\"]]`\n",
    "\n",
    "    Args:\n",
    "        corpus: List[str] - 待处理的语料\n",
    "\n",
    "    Return:\n",
    "        List[List[str]] - 二维List，内部的List由每个句子的单词str构成\n",
    "    \"\"\"\n",
    "\n",
    "    tokeneds = [list(filter(lambda token: len(token) > 0, _splitor_pattern.split(_digit_pattern.sub(\"N\", sentence.lower())))) for sentence in corpus]\n",
    "\n",
    "    return tokeneds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3732502a",
   "metadata": {},
   "source": [
    "编写相应函数构建BPE算法需要用到的初始状态词典。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7bf823e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'i </w>': 2, 'a m </w>': 1, 'h a p p y </w>': 1, 'h a v e </w>': 1, 'n </w>': 1, 'a p p l e s </w>': 1}\n"
     ]
    }
   ],
   "source": [
    "def build_bpe_vocab(corpus: List[str]) -> Dict[str, int]:\n",
    "    \"\"\"\n",
    "    将语料进行white_space_tokenize处理后，将单词每个字母以空格隔开、结尾加上</w>后，构建带频数的字典，例如：  \n",
    "    输入 `corpus = [\"I am happy.\", \"I have 10 apples!\"]`，  \n",
    "    得到\n",
    "    ```python\n",
    "    {\n",
    "        'i </w>': 2,\n",
    "        'a m </w>': 1,\n",
    "        'h a p p y </w>': 1,\n",
    "        'h a v e </w>': 1,\n",
    "        'N </w>': 1,\n",
    "        'a p p l e s </w>': 1\n",
    "    }\n",
    "    ```\n",
    "\n",
    "    Args:\n",
    "        corpus: List[str] - 待处理的语料\n",
    "\n",
    "    Return:\n",
    "        Dict[str, int] - \"单词分词状态->频数\"的词典\n",
    "    \"\"\"\n",
    "\n",
    "    tokenized_corpus = white_space_tokenize(corpus)\n",
    "\n",
    "    bpe_vocab = dict()\n",
    "\n",
    "    # TODO: 完成函数体【1分】\n",
    "    for sentence in tokenized_corpus:\n",
    "        for word in sentence:\n",
    "            # 将单词每个字母以空格隔开，并在结尾加上</w>\n",
    "            bpe_word = ' '.join(word.lower()) + ' </w>'\n",
    "            # 更新词典频数\n",
    "            bpe_vocab[bpe_word] = bpe_vocab.get(bpe_word, 0) + 1\n",
    "\n",
    "    return bpe_vocab\n",
    "\n",
    "print(build_bpe_vocab([\"I am happy.\", \"I have 10 apples!\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d25245",
   "metadata": {},
   "source": [
    "编写所需的其他函数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "087d11e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('i', '</w>'): 2, ('a', 'm'): 1, ('m', '</w>'): 1, ('h', 'a'): 2, ('a', 'p'): 2, ('p', 'p'): 2, ('p', 'y'): 1, ('y', '</w>'): 1, ('a', 'v'): 1, ('v', 'e'): 1, ('e', '</w>'): 1, ('n', '</w>'): 1, ('p', 'l'): 1, ('l', 'e'): 1, ('e', 's'): 1, ('s', '</w>'): 1}\n"
     ]
    }
   ],
   "source": [
    "def get_bigram_freq(bpe_vocab: Dict[str, int]) -> Dict[Tuple[str, str], int]:\n",
    "    \"\"\"\n",
    "    统计\"单词分词状态->频数\"的词典中，各bigram的频次（假设该词典中，各个unigram以空格间隔），例如：  \n",
    "    输入 \n",
    "    ```python\n",
    "    bpe_vocab = {\n",
    "        'i </w>': 2,\n",
    "        'a m </w>': 1,\n",
    "        'h a p p y </w>': 1,\n",
    "        'h a v e </w>': 1,\n",
    "        'N </w>': 1,\n",
    "        'a p p l e s </w>': 1\n",
    "    }\n",
    "    ```\n",
    "    得到\n",
    "    ```python\n",
    "    {\n",
    "        ('i', '</w>'): 2,\n",
    "        ('a', 'm'): 1,\n",
    "        ('m', '</w>'): 1,\n",
    "        ('h', 'a'): 2,\n",
    "        ('a', 'p'): 2,\n",
    "        ('p', 'p'): 2,\n",
    "        ('p', 'y'): 1,\n",
    "        ('y', '</w>'): 1,\n",
    "        ('a', 'v'): 1,\n",
    "        ('v', 'e'): 1,\n",
    "        ('e', '</w>'): 1,\n",
    "        ('N', '</w>'): 1,\n",
    "        ('p', 'l'): 1,\n",
    "        ('l', 'e'): 1,\n",
    "        ('e', 's'): 1,\n",
    "        ('s', '</w>'): 1\n",
    "    }\n",
    "    ```\n",
    "\n",
    "    Args:\n",
    "        bpe_vocab: Dict[str, int] - \"单词分词状态->频数\"的词典\n",
    "\n",
    "    Return:\n",
    "        Dict[Tuple[str, str], int] - \"bigram->频数\"的词典\n",
    "    \"\"\"\n",
    "\n",
    "    bigram_freq = dict()\n",
    "\n",
    "    # TODO: 完成函数体【1分】\n",
    "    for word, freq in bpe_vocab.items():\n",
    "        words = word.split(' ')\n",
    "        for i in range(len(words) - 1):\n",
    "            bigram = (words[i], words[i + 1])\n",
    "            bigram_freq[bigram] = bigram_freq.get(bigram, 0) + freq\n",
    "\n",
    "    return bigram_freq\n",
    "\n",
    "print(get_bigram_freq(build_bpe_vocab([\"I am happy.\", \"I have 10 apples!\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ba426043",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'i</w>': 2, 'a m </w>': 1, 'h a p p y </w>': 1, 'h a v e </w>': 1, 'n </w>': 1, 'a p p l e s </w>': 1}\n"
     ]
    }
   ],
   "source": [
    "def refresh_bpe_vocab_by_merging_bigram(bigram: Tuple[str, str], old_bpe_vocab: Dict[str, int]) -> Dict[str, int]:\n",
    "    \"\"\"\n",
    "    在\"单词分词状态->频数\"的词典中，合并指定的bigram（即去掉对应的相邻unigram之间的空格），最后返回新的词典，例如：  \n",
    "    输入 \n",
    "    ```python\n",
    "    bigram = ('i', '</w>'), old_bpe_vocab = {\n",
    "        'i </w>': 2,\n",
    "        'a m </w>': 1,\n",
    "        'h a p p y </w>': 1,\n",
    "        'h a v e </w>': 1,\n",
    "        'N </w>': 1,\n",
    "        'a p p l e s </w>': 1\n",
    "    }\n",
    "    ```\n",
    "    得到\n",
    "    ```python\n",
    "    {\n",
    "        'i</w>': 2,\n",
    "        'a m </w>': 1,\n",
    "        'h a p p y </w>': 1,\n",
    "        'h a v e </w>': 1,\n",
    "        'N </w>': 1,\n",
    "        'a p p l e s </w>': 1\n",
    "    }\n",
    "    ```\n",
    "    \n",
    "    Args:\n",
    "        bigram: Tuple[str, str] - 待合并的bigram\n",
    "        old_bpe_vocab: Dict[str, int] - 初始\"单词分词状态->频数\"的词典\n",
    "\n",
    "    Return:\n",
    "        Dict[str, int] - 合并后的\"单词分词状态->频数\"的词典\n",
    "    \"\"\"\n",
    "\n",
    "    new_bpe_vocab = dict()\n",
    "\n",
    "    # TODO: 完成函数体【1分】\n",
    "    temp = bigram[0] + ' ' + bigram[1]\n",
    "    for key, value in old_bpe_vocab.items():\n",
    "        if temp in key:\n",
    "            new_bpe_vocab[key.replace(temp, ''.join(bigram))] = value\n",
    "        else:\n",
    "            new_bpe_vocab[key] = value\n",
    "\n",
    "    return new_bpe_vocab\n",
    "\n",
    "print(refresh_bpe_vocab_by_merging_bigram(('i', '</w>'), build_bpe_vocab([\"I am happy.\", \"I have 10 apples!\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "992438a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('i</w>', 2), ('ha', 2), ('pp', 2), ('a', 2), ('m', 1), ('</w>', 5), ('y', 1), ('v', 1), ('e', 2), ('N', 1), ('l', 1), ('s', 1)]\n"
     ]
    }
   ],
   "source": [
    "def get_bpe_tokens(bpe_vocab: Dict[str, int]) -> List[Tuple[str, int]]:\n",
    "    \"\"\"\n",
    "    根据\"单词分词状态->频数\"的词典，返回所得到的BPE分词列表，并将该列表按照分词长度降序排序返回，例如：  \n",
    "    输入 \n",
    "    ```python\n",
    "    bpe_vocab = {\n",
    "        'i</w>': 2,\n",
    "        'a m </w>': 1,\n",
    "        'ha pp y </w>': 1,\n",
    "        'ha v e </w>': 1,\n",
    "        'N </w>': 1,\n",
    "        'a pp l e s </w>': 1\n",
    "    }\n",
    "    ```\n",
    "    得到\n",
    "    ```\n",
    "    [\n",
    "        ('i</w>', 2),\n",
    "        ('ha', 2),\n",
    "        ('pp', 2),\n",
    "        ('a', 2),\n",
    "        ('m', 1),\n",
    "        ('</w>', 5),\n",
    "        ('y', 1),\n",
    "        ('v', 1),\n",
    "        ('e', 2),\n",
    "        ('N', 1),\n",
    "        ('l', 1),\n",
    "        ('s', 1)\n",
    "    ]\n",
    "    ```\n",
    "\n",
    "    Args:\n",
    "        bpe_vocab: Dict[str, int] - \"单词分词状态->频数\"的词典\n",
    "\n",
    "    Return:\n",
    "        List[Tuple[str, int]] - BPE分词和对应频数组成的List\n",
    "    \"\"\"\n",
    "\n",
    "    bpe_tokens = []\n",
    "    # TODO: 完成函数体【2分】\n",
    "    bpe_tokens_dict = dict()\n",
    "    for key, freq in bpe_vocab.items():\n",
    "        for token in key.split():\n",
    "            if token in bpe_tokens_dict.keys():\n",
    "                bpe_tokens_dict[token] += freq\n",
    "            else:\n",
    "                bpe_tokens_dict[token] = freq\n",
    "\n",
    "    bpe_tokens = sorted(bpe_tokens_dict.items(), key=lambda item: len(item[0]) - 3 if \"</w>\" in item[0]  else len(item[0]), reverse = True)\n",
    "\n",
    "    return bpe_tokens\n",
    "\n",
    "print(get_bpe_tokens({\n",
    "        'i</w>': 2,\n",
    "        'a m </w>': 1,\n",
    "        'ha pp y </w>': 1,\n",
    "        'ha v e </w>': 1,\n",
    "        'N </w>': 1,\n",
    "        'a pp l e s </w>': 1\n",
    "    }))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3c56995e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "su per <unknown>\n",
      "sh an g hai </w>\n"
     ]
    }
   ],
   "source": [
    "def print_bpe_tokenize(word: str, bpe_tokens: List[Tuple[str, int]]):\n",
    "    \"\"\"\n",
    "    根据按长度降序的BPE分词列表，将所给单词进行BPE分词，最后打印结果。\n",
    "    \n",
    "    思想是，对于一个待BPE分词的单词，按照长度顺序从列表中寻找BPE分词进行子串匹配，  \n",
    "    若成功匹配，则对该子串左右的剩余部分递归地进行下一轮匹配，直到剩余部分长度为0，  \n",
    "    或者剩余部分无法匹配（该部分整体由`\"<unknown>\"`代替）\n",
    "    \n",
    "    例1：  \n",
    "    输入 `word = \"supermarket\"`, `bpe_tokens=[\n",
    "        (\"su\", 20),\n",
    "        (\"are\", 10),\n",
    "        (\"per\", 30),\n",
    "    ]`  \n",
    "    最终打印 `\"su per <unknown>\"`\n",
    "\n",
    "    例2：  \n",
    "    输入 `word = \"shanghai\"`, `bpe_tokens=[\n",
    "        (\"hai\", 1),\n",
    "        (\"sh\", 1),\n",
    "        (\"an\", 1),\n",
    "        (\"</w>\", 1),\n",
    "        (\"g\", 1)\n",
    "    ]`  \n",
    "    最终打印 `\"sh an g hai </w>\"`\n",
    "\n",
    "    Args:\n",
    "        word: str - 待分词的单词\n",
    "        bpe_tokens: List[Tuple(str, int)] - BPE分词和对应频数组成的列表\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: 请尝试使用递归函数定义该分词过程【2分】\n",
    "    def bpe_tokenize(sub_word: str) -> str:\n",
    "        if not sub_word:\n",
    "            return \"\"\n",
    "\n",
    "        for token, _ in bpe_tokens:\n",
    "            if token in sub_word:\n",
    "                index = sub_word.find(token)\n",
    "                left = bpe_tokenize(sub_word[:index])\n",
    "                left = left + ' ' if left else \"\"\n",
    "                right = bpe_tokenize(sub_word[index + len(token):])\n",
    "                right = ' ' + right if right else \"\"\n",
    "                return left + token + right\n",
    "\n",
    "        return \"<unknown>\"\n",
    "\n",
    "    res = bpe_tokenize(word + \"</w>\")\n",
    "    print(res)\n",
    "\n",
    "print_bpe_tokenize(\"supermarket\", [\n",
    "        (\"su\", 20),\n",
    "        (\"are\", 10),\n",
    "        (\"per\", 30),\n",
    "    ])\n",
    "print_bpe_tokenize(\"shanghai\", [\n",
    "        (\"hai\", 1),\n",
    "        (\"sh\", 1),\n",
    "        (\"an\", 1),\n",
    "        (\"</w>\", 1),\n",
    "        (\"g\", 1)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd70402",
   "metadata": {},
   "source": [
    "开始读取数据集并训练BPE分词器："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "215b56d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded training corpus.\n"
     ]
    }
   ],
   "source": [
    "with open(\"../../NLPhomework1data/news.2007.en.shuffled.deduped.train\", encoding=\"utf-8\") as f:\n",
    "    training_corpus = list(map(lambda l: l.strip(), f.readlines()[:1000]))\n",
    "\n",
    "print(\"Loaded training corpus.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7bccd41d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('e', '</w>')\n",
      "('s', '</w>')\n",
      "('n', '</w>')\n",
      "('t', 'h')\n",
      "('d', '</w>')\n",
      "('t', '</w>')\n",
      "('e', 'r')\n",
      "('i', 'n')\n",
      "('th', 'e</w>')\n",
      "('a', 'n')\n",
      "('y', '</w>')\n",
      "('o', 'r')\n",
      "('a', 'r')\n",
      "('o', '</w>')\n",
      "('e', 'd</w>')\n",
      "('e', 'n')\n",
      "('a', 'l')\n",
      "('g', '</w>')\n",
      "('a', '</w>')\n",
      "('r', 'e')\n",
      "('o', 'u')\n",
      "('o', 'n')\n",
      "('in', 'g</w>')\n",
      "('t', 'i')\n",
      "('f', '</w>')\n",
      "('er', '</w>')\n",
      "('t', 'o</w>')\n",
      "('an', 'd</w>')\n",
      "('o', 'f</w>')\n",
      "('s', 't')\n",
      "('o', 'm')\n",
      "('e', 's</w>')\n",
      "('i', 'c')\n",
      "('i', 'l')\n",
      "('c', 'h')\n",
      "('a', 't')\n",
      "('i', 's</w>')\n",
      "('a', 's</w>')\n",
      "('r', 'o')\n",
      "('e', 'l')\n",
      "('or', '</w>')\n",
      "('al', '</w>')\n",
      "('i', 't')\n",
      "('a', 'm')\n",
      "('s', 'h')\n",
      "('a', 'c')\n",
      "('l', '</w>')\n",
      "('a', 'i')\n",
      "('e', 'c')\n",
      "('w', 'h')\n",
      "('r', 'i')\n",
      "('e', 'm')\n",
      "('a', 's')\n",
      "('en', 't</w>')\n",
      "('o', 'l')\n",
      "('k', '</w>')\n",
      "('l', 'i')\n",
      "('s', 'i')\n",
      "('n', 'e')\n",
      "('s', 'u')\n",
      "('v', 'e</w>')\n",
      "('w', '</w>')\n",
      "('f', 'or</w>')\n",
      "('g', 'h')\n",
      "('p', 'l')\n",
      "('th', 'at</w>')\n",
      "('r', '</w>')\n",
      "('t', 's</w>')\n",
      "('a', 'd')\n",
      "(\"'\", 's</w>')\n",
      "('d', 'e')\n",
      "('u', 'n')\n",
      "('a', 'y</w>')\n",
      "('l', 'e')\n",
      "('c', 'om')\n",
      "('l', 'o')\n",
      "('d', 'i')\n",
      "('a', 'g')\n",
      "('c', 'on')\n",
      "('b', 'u')\n",
      "('l', 'y</w>')\n",
      "('w', 'ith')\n",
      "('s', 'e</w>')\n",
      "('ai', 'd</w>')\n",
      "('s', 'e')\n",
      "('p', 'ro')\n",
      "('a', 'b')\n",
      "('with', '</w>')\n",
      "('th', 'e')\n",
      "('p', '</w>')\n",
      "('s', 'aid</w>')\n",
      "('b', 'e')\n",
      "('n', 'o')\n",
      "('s', 'p')\n",
      "('v', 'i')\n",
      "('u', 'r')\n",
      "('c', 'e</w>')\n",
      "('a', 'p')\n",
      "('h', 'e</w>')\n",
      "('i', '</w>')\n",
      "('v', 'er')\n",
      "('e', 'x')\n",
      "('w', 'as</w>')\n",
      "('h', 'a')\n",
      "('t', 'r')\n",
      "('f', 'f')\n",
      "('s', 's</w>')\n",
      "('a', 'k')\n",
      "('f', 'i')\n",
      "('ati', 'on</w>')\n",
      "('er', 's</w>')\n",
      "('b', 'y</w>')\n",
      "('p', 'e')\n",
      "('p', 'o')\n",
      "('t', 'ed</w>')\n",
      "('t', 'o')\n",
      "('d', 's</w>')\n",
      "('h', 'is</w>')\n",
      "('f', 'rom')\n",
      "('from', '</w>')\n",
      "('l', 'd</w>')\n",
      "('m', '</w>')\n",
      "('m', 'o')\n",
      "('at', 'e</w>')\n",
      "('d', 'ay</w>')\n",
      "('t', 'er</w>')\n",
      "('th', '</w>')\n",
      "('f', 'or')\n",
      "('a', 'u')\n",
      "('ha', 've</w>')\n",
      "('w', 'e')\n",
      "('ou', 't</w>')\n",
      "('y', 'e')\n",
      "('q', 'u')\n",
      "('on', 's</w>')\n",
      "('ou', 'ld</w>')\n",
      "('w', 'or')\n",
      "('il', 'l</w>')\n",
      "('h', 'i')\n",
      "('s', 'c')\n",
      "('a', 'f')\n",
      "('the', 'y</w>')\n",
      "('gh', 't</w>')\n",
      "('m', 'ent</w>')\n",
      "('f', 'e')\n",
      "('p', 're')\n",
      "('ne', 'w</w>')\n",
      "('bu', 't</w>')\n",
      "('w', 'ill</w>')\n",
      "('d', 'u')\n",
      "('t', 'er')\n",
      "('s', 'o</w>')\n",
      "('g', 'e')\n",
      "('m', 'e')\n",
      "('p', 'ar')\n",
      "('b', 'o')\n",
      "('p', 'u')\n",
      "('al', 'l</w>')\n",
      "('ti', 'on</w>')\n",
      "('a', 'y')\n",
      "('d', 'o')\n",
      "('t', 'e')\n",
      "('g', 'o')\n",
      "('m', 'i')\n",
      "('r', 'u')\n",
      "('k', 'e')\n",
      "('h', 'e')\n",
      "('i', 'r</w>')\n",
      "('c', 'u')\n",
      "('g', 'i')\n",
      "('ch', '</w>')\n",
      "('wh', 'o</w>')\n",
      "('m', 'ar')\n",
      "('no', 't</w>')\n",
      "('the', 'ir</w>')\n",
      "('c', 'l')\n",
      "('j', 'u')\n",
      "('gh', '</w>')\n",
      "('h', 'o')\n",
      "('m', 'an')\n",
      "('w', 'n</w>')\n",
      "('g', 'u')\n",
      "('y', 'ou')\n",
      "('c', 'oun')\n",
      "('w', 'ould</w>')\n",
      "('you', '</w>')\n",
      "('c', 'o')\n",
      "('c', 'i')\n",
      "('pe', 'o')\n",
      "('v', 'e')\n",
      "('peo', 'ple</w>')\n",
      "(\"'\", 't</w>')\n",
      "('t', 'y</w>')\n",
      "('s', 'o')\n",
      "('ye', 'ar</w>')\n",
      "('g', 'r')\n",
      "('af', 'ter</w>')\n",
      "('al', 'so</w>')\n",
      "('on', 'al</w>')\n",
      "('m', 'u')\n",
      "('ar', 's</w>')\n",
      "('n', 'i')\n",
      "('al', 'ly</w>')\n",
      "('c', 'e')\n",
      "('k', 's</w>')\n",
      "('f', 'u')\n",
      "('ar', 'd</w>')\n",
      "('fi', 'r')\n",
      "('t', 'w')\n",
      "('ab', 'le</w>')\n",
      "('i', 'r')\n",
      "('i', 'm')\n",
      "('c', 'ar')\n",
      "('o', 'ther</w>')\n",
      "('be', 'en</w>')\n",
      "('c', 're')\n",
      "('an', 't</w>')\n",
      "('sh', '</w>')\n",
      "('si', 'on</w>')\n",
      "('in', 'to</w>')\n",
      "('ar', 'y</w>')\n",
      "('r', 'an')\n",
      "('c', 'an</w>')\n",
      "('w', 'ay</w>')\n",
      "('k', 'no')\n",
      "('j', 'o')\n",
      "('k', 'ing</w>')\n",
      "('ac', 'k</w>')\n",
      "('th', 're')\n",
      "('li', 'on</w>')\n",
      "('a', 'v')\n",
      "('x', '</w>')\n",
      "('tw', 'o</w>')\n",
      "('ay', 's</w>')\n",
      "('ati', 'ons</w>')\n",
      "('u', '</w>')\n",
      "('t', 'ur')\n",
      "('g', 's</w>')\n",
      "('cl', 'u')\n",
      "('b', '</w>')\n",
      "('u', 's</w>')\n",
      "('w', 's</w>')\n",
      "('e', 'ar')\n",
      "('i', 'f</w>')\n",
      "('l', 's</w>')\n",
      "('o', 'ver</w>')\n",
      "('in', 'ter')\n",
      "('por', 't</w>')\n",
      "('s', 'day</w>')\n",
      "('s', 'y')\n",
      "('ver', 'y</w>')\n",
      "('an', 'y</w>')\n",
      "('p', 's</w>')\n",
      "('h', 'u')\n",
      "('mo', 'st</w>')\n",
      "('g', 'n</w>')\n",
      "('an', 's</w>')\n",
      "('m', 'r</w>')\n",
      "('n', 'ati')\n",
      "('fir', 'st</w>')\n",
      "('c', 'an')\n",
      "('i', 's')\n",
      "('u', 'p</w>')\n",
      "('ge', 't</w>')\n",
      "('mil', 'lion</w>')\n",
      "('j', 'ec')\n",
      "('g', 'y</w>')\n",
      "('t', 'ly</w>')\n",
      "('du', 'c')\n",
      "('t', 'u')\n",
      "('n', 'u')\n",
      "('e', 't</w>')\n",
      "('l', 'an')\n",
      "('nati', 'onal</w>')\n",
      "('d', 'y</w>')\n",
      "('f', 'ri')\n",
      "('thre', 'e</w>')\n",
      "('li', 'ke</w>')\n",
      "('pre', 'si')\n",
      "('st', 'ar')\n",
      "('in', 've')\n",
      "('b', 'i')\n",
      "('in', 'clu')\n",
      "('ag', 'ain')\n",
      "('again', 'st</w>')\n",
      "('on', 'ly</w>')\n",
      "('presi', 'dent</w>')\n",
      "('per', 'cent</w>')\n",
      "('v', 'o')\n",
      "('b', 'ack</w>')\n",
      "('bu', 'sine')\n",
      "('c', '</w>')\n",
      "('c', 'al')\n",
      "('o', 'd</w>')\n",
      "('au', 'se</w>')\n",
      "('en', 'd</w>')\n",
      "('t', 'al</w>')\n",
      "('com', 'p')\n",
      "('p', 'i')\n",
      "('m', 'ake</w>')\n"
     ]
    }
   ],
   "source": [
    "training_iter_num = 300\n",
    "\n",
    "training_bpe_vocab = build_bpe_vocab(training_corpus)\n",
    "for i in range(training_iter_num):\n",
    "    # TODO: 完成训练循环内的代码逻辑【2分】\n",
    "    mostFreq = sorted(get_bigram_freq(training_bpe_vocab).items(), key = lambda item: item[1], reverse = True)[0][0]\n",
    "    print(mostFreq)\n",
    "    training_bpe_vocab = refresh_bpe_vocab_by_merging_bigram(mostFreq, training_bpe_vocab)\n",
    "\n",
    "\n",
    "training_bpe_tokens = get_bpe_tokens(training_bpe_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ea3ddd",
   "metadata": {},
   "source": [
    "测试BPE分词器的分词效果："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c0cfdb29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naturallanguageprocessing 的分词结果为：\n",
      "n atur al lan gu age pro ce s sing</w>\n"
     ]
    }
   ],
   "source": [
    "test_word = \"naturallanguageprocessing\"\n",
    "\n",
    "print(\"naturallanguageprocessing 的分词结果为：\")\n",
    "print_bpe_tokenize(test_word, training_bpe_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e5f2a1",
   "metadata": {},
   "source": [
    "### 实验总结\n",
    "> TODO：请在这里填写实验总结。\n",
    "修改了数据集路径\n",
    "按照题目的要求来，思路非常清晰"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
