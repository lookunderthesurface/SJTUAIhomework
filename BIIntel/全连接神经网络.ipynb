{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4519b03b",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (1067174777.py, line 40)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[1], line 40\u001b[0;36m\u001b[0m\n\u001b[0;31m    functional.set_step_mode(self, step_mode='m')\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import argparse\n",
    "import sys\n",
    "import datetime\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "from torch import amp\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torchvision\n",
    "import numpy as np\n",
    "\n",
    "from spikingjelly.activation_based import neuron, encoding, functional, surrogate, layer\n",
    "from ColorMNIST import ColorMNIST\n",
    "\n",
    "path_to_mnist = '/workspace/mnist_data/MNIST'\n",
    "\n",
    "#定义SNN网络结构\n",
    "class SNN(nn.Module):\n",
    "    def __init__(self, tau):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layer = nn.Sequential(\n",
    "            # layer.Flatten(),\n",
    "            # layer.Linear(28 * 28, 10, bias=False),\n",
    "            layer.Conv2d(3, 128, kernel_size=3, padding=1, bias=False),    #普通卷积层\n",
    "            layer.BatchNorm2d(128),\n",
    "            #neuron.LIFNode(tau=tau, surrogate_function=surrogate.ATan()),\n",
    "            layer.MaxPool2d(4, 4),\n",
    "\n",
    "            layer.Flatten(),\n",
    "            layer.Linear(7 * 7 * 128, 10, bias=False),\n",
    "            #SNN LIF Node\n",
    "            neuron.LIFNode(tau=tau, surrogate_function=surrogate.ATan()),\n",
    "            )\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return self.layer(x)\n",
    "\n",
    "def main():\n",
    "    '''\n",
    "    :return: None\n",
    "\n",
    "    * :ref:`API in English <lif_fc_mnist.main-en>`\n",
    "\n",
    "    .. _lif_fc_mnist.main-cn:\n",
    "\n",
    "    使用全连接-LIF的网络结构，进行MNIST识别。\\n\n",
    "    这个函数会初始化网络进行训练，并显示训练过程中在测试集的正确率。\n",
    "\n",
    "    * :ref:`中文API <lif_fc_mnist.main-cn>`\n",
    "\n",
    "    .. _lif_fc_mnist.main-en:\n",
    "\n",
    "    The network with FC-LIF structure for classifying MNIST.\\n\n",
    "    This function initials the network, starts trainingand shows accuracy on test dataset.\n",
    "    '''\n",
    "    class parser:\n",
    "        def __init__(self):\n",
    "            self.T = 100\n",
    "            self.device='cuda:0'#cpu'\n",
    "            self.epochs = 100\n",
    "            self.b = 64\n",
    "            self.j=4\n",
    "            self.data_dir='/workspace/mnist_data'\n",
    "            self.out_dir='./logs'\n",
    "            self.resume = None\n",
    "            self.amp = True#False\n",
    "            self.opt = 'adam'\n",
    "            self.lr=1e-3\n",
    "            self.tau=2.0\n",
    "    '''        \n",
    "    parser = argparse.ArgumentParser(description='LIF MNIST Training')\n",
    "    parser.add_argument('-T', default=100, type=int, help='simulating time-steps')\n",
    "    parser.add_argument('-device', default='cuda:0', help='device')\n",
    "    parser.add_argument('-b', default=64, type=int, help='batch size')\n",
    "    parser.add_argument('-epochs', default=100, type=int, metavar='N',\n",
    "                        help='number of total epochs to run')\n",
    "    parser.add_argument('-j', default=4, type=int, metavar='N',\n",
    "                        help='number of data loading workers (default: 4)')\n",
    "    parser.add_argument('-data-dir', type=str, help='root dir of MNIST dataset')\n",
    "    parser.add_argument('-out-dir', type=str, default='./logs', help='root dir for saving logs and checkpoint')\n",
    "    parser.add_argument('-resume', type=str, help='resume from the checkpoint path')\n",
    "    parser.add_argument('-amp', action='store_true', help='automatic mixed precision training')\n",
    "    parser.add_argument('-opt', type=str, choices=['sgd', 'adam'], default='adam', help='use which optimizer. SGD or Adam')\n",
    "    parser.add_argument('-momentum', default=0.9, type=float, help='momentum for SGD')\n",
    "    parser.add_argument('-lr', default=1e-3, type=float, help='learning rate')\n",
    "    parser.add_argument('-tau', default=2.0, type=float, help='parameter tau of LIF neuron')\n",
    "    '''\n",
    "    args = parser()\n",
    "    #args = parser.parse_args()\n",
    "    #print(args)\n",
    "\n",
    "    net = SNN(tau=args.tau)\n",
    "\n",
    "    print(net)\n",
    "\n",
    "    net.to(args.device)\n",
    "\n",
    "    train_dataset = ColorMNIST('both', 'train', path_to_mnist, randomcolor=True)\n",
    "    test_dataset = ColorMNIST('both', 'test', path_to_mnist, randomcolor=True)\n",
    "    # 初始化数据加载器\n",
    "    # train_dataset = torchvision.datasets.MNIST(\n",
    "    #     root=args.data_dir,\n",
    "    #     train=True,\n",
    "    #     transform=torchvision.transforms.ToTensor(),\n",
    "    #     download=True\n",
    "    # )\n",
    "    # test_dataset = torchvision.datasets.MNIST(\n",
    "    #     root=args.data_dir,\n",
    "    #     train=False,\n",
    "    #     transform=torchvision.transforms.ToTensor(),\n",
    "    #     download=True\n",
    "    # )\n",
    "\n",
    "    train_data_loader = data.DataLoader(\n",
    "        dataset=train_dataset,\n",
    "        batch_size=args.b,\n",
    "        shuffle=True,\n",
    "        drop_last=True,\n",
    "        num_workers=args.j,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    test_data_loader = data.DataLoader(\n",
    "        dataset=test_dataset,\n",
    "        batch_size=args.b,\n",
    "        shuffle=False,\n",
    "        drop_last=False,\n",
    "        num_workers=args.j,\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "    scaler = None\n",
    "    if args.amp:\n",
    "        scaler = amp.GradScaler()\n",
    "\n",
    "    start_epoch = 0\n",
    "    max_test_acc = -1\n",
    "\n",
    "    optimizer = None\n",
    "    if args.opt == 'sgd':\n",
    "        optimizer = torch.optim.SGD(net.parameters(), lr=args.lr, momentum=args.momentum)\n",
    "    elif args.opt == 'adam':\n",
    "        optimizer = torch.optim.Adam(net.parameters(), lr=args.lr)\n",
    "    else:\n",
    "        raise NotImplementedError(args.opt)\n",
    "\n",
    "    if args.resume:\n",
    "        checkpoint = torch.load(args.resume, map_location='cpu')\n",
    "        net.load_state_dict(checkpoint['net'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        start_epoch = checkpoint['epoch'] + 1\n",
    "        max_test_acc = checkpoint['max_test_acc']\n",
    "    \n",
    "    out_dir = os.path.join(args.out_dir, f'T{args.T}_b{args.b}_{args.opt}_lr{args.lr}')\n",
    "\n",
    "    if args.amp:\n",
    "        out_dir += '_amp'\n",
    "\n",
    "    if not os.path.exists(out_dir):\n",
    "        os.makedirs(out_dir)\n",
    "        print(f'Mkdir {out_dir}.')\n",
    "\n",
    "    with open(os.path.join(out_dir, 'args.txt'), 'w', encoding='utf-8') as args_txt:\n",
    "        args_txt.write(str(args))\n",
    "\n",
    "    writer = SummaryWriter(out_dir, purge_step=start_epoch)\n",
    "    with open(os.path.join(out_dir, 'args.txt'), 'w', encoding='utf-8') as args_txt:\n",
    "        args_txt.write(str(args))\n",
    "        args_txt.write('\\n')\n",
    "        args_txt.write(' '.join(sys.argv))\n",
    "\n",
    "    #使用Poisson编码器对输入进行编码\n",
    "    encoder = encoding.PoissonEncoder()\n",
    "\n",
    "    for epoch in range(start_epoch, args.epochs):\n",
    "        start_time = time.time()\n",
    "        net.train()\n",
    "        train_loss = 0\n",
    "        train_acc = 0\n",
    "        train_samples = 0\n",
    "        for img, label in train_data_loader:\n",
    "            optimizer.zero_grad()\n",
    "            img = img.to(args.device)\n",
    "            label = label.to(args.device)\n",
    "            label_onehot = F.one_hot(label, 10).float() #对输出进行one-hot编码\n",
    "\n",
    "            if scaler is not None:\n",
    "                with amp.autocast(device_type='cuda'): #使用amp混合精度（Cuda)计算可以加速\n",
    "                    out_fr = 0. \n",
    "                    for t in range(args.T): #SNN内部T步时间步长\n",
    "                        encoded_img = encoder(img)\n",
    "                        out_fr += net(encoded_img)  #累加输出的脉冲个数\n",
    "                    out_fr = out_fr / args.T #输出脉冲个数归一化\n",
    "                    loss = F.mse_loss(out_fr, label_onehot) #最小化SNN预测值和one-hot编码的区别\n",
    "                scaler.scale(loss).backward() #使用混合精度计算计算loss\n",
    "                scaler.step(optimizer)        #使用混合精度计算进行optimization\n",
    "                scaler.update()\n",
    "            else:   #同上\n",
    "                out_fr = 0. \n",
    "                for t in range(args.T):\n",
    "                    encoded_img = encoder(img)\n",
    "                    out_fr += net(encoded_img)\n",
    "                out_fr = out_fr / args.T\n",
    "                loss = F.mse_loss(out_fr, label_onehot)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            train_samples += label.numel()\n",
    "            train_loss += loss.item() * label.numel()\n",
    "            train_acc += (out_fr.argmax(1) == label).float().sum().item()  #计算训练精确度\n",
    "\n",
    "            functional.reset_net(net) #注意：此步为必要步骤，让LIF node的membrane potential回到0\n",
    "\n",
    "        train_time = time.time()\n",
    "        train_speed = train_samples / (train_time - start_time)\n",
    "        train_loss /= train_samples\n",
    "        train_acc /= train_samples\n",
    "\n",
    "        writer.add_scalar('train_loss', train_loss, epoch)\n",
    "        writer.add_scalar('train_acc', train_acc, epoch)\n",
    "\n",
    "        net.eval()\n",
    "        test_loss = 0\n",
    "        test_acc = 0\n",
    "        test_samples = 0\n",
    "        with torch.no_grad():\n",
    "            for img, label in test_data_loader:  #类似训练步骤计算测试数据集上的性能\n",
    "                img = img.to(args.device)\n",
    "                label = label.to(args.device)\n",
    "                label_onehot = F.one_hot(label, 10).float()\n",
    "                out_fr = 0.\n",
    "                for t in range(args.T):\n",
    "                    encoded_img = encoder(img)\n",
    "                    out_fr += net(encoded_img)\n",
    "                out_fr = out_fr / args.T\n",
    "                loss = F.mse_loss(out_fr, label_onehot)\n",
    "\n",
    "                test_samples += label.numel()\n",
    "                test_loss += loss.item() * label.numel()\n",
    "                test_acc += (out_fr.argmax(1) == label).float().sum().item()\n",
    "                functional.reset_net(net)\n",
    "        test_time = time.time()\n",
    "        test_speed = test_samples / (test_time - train_time)\n",
    "        test_loss /= test_samples\n",
    "        test_acc /= test_samples\n",
    "        writer.add_scalar('test_loss', test_loss, epoch)\n",
    "        writer.add_scalar('test_acc', test_acc, epoch)\n",
    "\n",
    "        save_max = False\n",
    "        if test_acc > max_test_acc:\n",
    "            max_test_acc = test_acc\n",
    "            save_max = True\n",
    "\n",
    "        checkpoint = {\n",
    "            'net': net.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'epoch': epoch,\n",
    "            'max_test_acc': max_test_acc\n",
    "        }\n",
    "\n",
    "        if save_max:\n",
    "            torch.save(checkpoint, os.path.join(out_dir, 'checkpoint_max.pth'))\n",
    "\n",
    "        torch.save(checkpoint, os.path.join(out_dir, 'checkpoint_latest.pth'))\n",
    "\n",
    "        print(args)\n",
    "        print(out_dir)\n",
    "        print(f'epoch ={epoch}, train_loss ={train_loss: .4f}, train_acc ={train_acc: .4f}, test_loss ={test_loss: .4f}, test_acc ={test_acc: .4f}, max_test_acc ={max_test_acc: .4f}')\n",
    "        print(f'train speed ={train_speed: .4f} images/s, test speed ={test_speed: .4f} images/s')\n",
    "        print(f'escape time = {(datetime.datetime.now() + datetime.timedelta(seconds=(time.time() - start_time) * (args.epochs - epoch))).strftime(\"%Y-%m-%d %H:%M:%S\")}\\n')\n",
    "\n",
    "    # 保存绘图用数据\n",
    "    net.eval()\n",
    "    # 注册钩子\n",
    "    output_layer = net.layer[-1] # 输出层\n",
    "    output_layer.v_seq = []\n",
    "    output_layer.s_seq = []\n",
    "    def save_hook(m, x, y):\n",
    "        m.v_seq.append(m.v.unsqueeze(0))\n",
    "        m.s_seq.append(y.unsqueeze(0))\n",
    "\n",
    "    output_layer.register_forward_hook(save_hook)\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        img, label = test_dataset[0]\n",
    "        img = img.to(args.device)\n",
    "        out_fr = 0.\n",
    "        for t in range(args.T):\n",
    "            encoded_img = encoder(img)\n",
    "            encoded_img = encoded_img.unsqueeze(0)\n",
    "            print(\"encoded_img.shape is \",encoded_img.shape)\n",
    "            out_fr += net(encoded_img)\n",
    "        out_spikes_counter_frequency = (out_fr / args.T).cpu().numpy()\n",
    "        print(f'Firing rate: {out_spikes_counter_frequency}')\n",
    "\n",
    "        output_layer.v_seq = torch.cat(output_layer.v_seq)\n",
    "        output_layer.s_seq = torch.cat(output_layer.s_seq)\n",
    "        v_t_array = output_layer.v_seq.cpu().numpy().squeeze()  # v_t_array[i][j]表示神经元i在j时刻的电压值\n",
    "        np.save(\"v_t_array.npy\",v_t_array)\n",
    "        s_t_array = output_layer.s_seq.cpu().numpy().squeeze()  # s_t_array[i][j]表示神经元i在j时刻释放的脉冲，为0或1\n",
    "        np.save(\"s_t_array.npy\",s_t_array)\n",
    "\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9383e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc006d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "epochs = range(1, 20)\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs, train_loss, label='Train Loss')\n",
    "plt.plot(epochs, test_loss, label='Test Loss')\n",
    "plt.title('Loss vs. Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "# 绘制训练、测试准确率和最大测试准确率\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs, train_acc, label='Train Accuracy')\n",
    "plt.plot(epochs, test_acc, label='Test Accuracy')\n",
    "plt.plot(epochs, max_test_acc, label='Max Test Accuracy', linestyle='--')\n",
    "plt.title('Accuracy vs. Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d56e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(v_t_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896e199d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
