{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4519b03b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SNN(\n",
      "  (layer): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1, step_mode=s)\n",
      "    (1): Linear(in_features=784, out_features=10, bias=False)\n",
      "    (2): LIFNode(\n",
      "      v_threshold=1.0, v_reset=0.0, detach_reset=False, step_mode=s, backend=torch, tau=2.0\n",
      "      (surrogate_function): ATan(alpha=2.0, spiking=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1131)>\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./mnist_data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 9912422/9912422 [03:25<00:00, 48220.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./mnist_data/MNIST/raw/train-images-idx3-ubyte.gz to ./mnist_data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1131)>\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./mnist_data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 28881/28881 [00:00<00:00, 33826.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./mnist_data/MNIST/raw/train-labels-idx1-ubyte.gz to ./mnist_data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1131)>\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./mnist_data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1648877/1648877 [00:32<00:00, 50623.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./mnist_data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./mnist_data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1131)>\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./mnist_data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4542/4542 [00:00<00:00, 134949.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./mnist_data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./mnist_data/MNIST/raw\n",
      "\n",
      "Mkdir ./logs/T100_b64_adam_lr0.001.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.main.<locals>.parser object at 0x281e268e0>\n",
      "./logs/T100_b64_adam_lr0.001\n",
      "epoch =0, train_loss = 0.0274, train_acc = 0.8686, test_loss = 0.0193, test_acc = 0.9098, max_test_acc = 0.9098\n",
      "train speed = 1738.1365 images/s, test speed = 1701.2103 images/s\n",
      "escape time = 2024-11-07 21:17:31\n",
      "\n",
      "<__main__.main.<locals>.parser object at 0x281e268e0>\n",
      "./logs/T100_b64_adam_lr0.001\n",
      "epoch =1, train_loss = 0.0189, train_acc = 0.9105, test_loss = 0.0172, test_acc = 0.9170, max_test_acc = 0.9170\n",
      "train speed = 1785.0123 images/s, test speed = 1705.4651 images/s\n",
      "escape time = 2024-11-07 21:15:59\n",
      "\n",
      "<__main__.main.<locals>.parser object at 0x281e268e0>\n",
      "./logs/T100_b64_adam_lr0.001\n",
      "epoch =2, train_loss = 0.0175, train_acc = 0.9149, test_loss = 0.0166, test_acc = 0.9196, max_test_acc = 0.9196\n",
      "train speed = 1820.0957 images/s, test speed = 1672.2204 images/s\n",
      "escape time = 2024-11-07 21:15:06\n",
      "\n",
      "<__main__.main.<locals>.parser object at 0x281e268e0>\n",
      "./logs/T100_b64_adam_lr0.001\n",
      "epoch =3, train_loss = 0.0169, train_acc = 0.9185, test_loss = 0.0164, test_acc = 0.9184, max_test_acc = 0.9196\n",
      "train speed = 1819.5002 images/s, test speed = 1628.1154 images/s\n",
      "escape time = 2024-11-07 21:15:23\n",
      "\n",
      "<__main__.main.<locals>.parser object at 0x281e268e0>\n",
      "./logs/T100_b64_adam_lr0.001\n",
      "epoch =4, train_loss = 0.0164, train_acc = 0.9206, test_loss = 0.0159, test_acc = 0.9213, max_test_acc = 0.9213\n",
      "train speed = 1776.7011 images/s, test speed = 1641.2904 images/s\n",
      "escape time = 2024-11-07 21:16:35\n",
      "\n",
      "<__main__.main.<locals>.parser object at 0x281e268e0>\n",
      "./logs/T100_b64_adam_lr0.001\n",
      "epoch =5, train_loss = 0.0161, train_acc = 0.9217, test_loss = 0.0159, test_acc = 0.9211, max_test_acc = 0.9213\n",
      "train speed = 1810.6215 images/s, test speed = 1719.3144 images/s\n",
      "escape time = 2024-11-07 21:15:08\n",
      "\n",
      "<__main__.main.<locals>.parser object at 0x281e268e0>\n",
      "./logs/T100_b64_adam_lr0.001\n",
      "epoch =6, train_loss = 0.0158, train_acc = 0.9230, test_loss = 0.0156, test_acc = 0.9235, max_test_acc = 0.9235\n",
      "train speed = 1796.8807 images/s, test speed = 1614.6783 images/s\n",
      "escape time = 2024-11-07 21:16:08\n",
      "\n",
      "<__main__.main.<locals>.parser object at 0x281e268e0>\n",
      "./logs/T100_b64_adam_lr0.001\n",
      "epoch =7, train_loss = 0.0156, train_acc = 0.9238, test_loss = 0.0153, test_acc = 0.9229, max_test_acc = 0.9235\n",
      "train speed = 1737.8346 images/s, test speed = 1633.4298 images/s\n",
      "escape time = 2024-11-07 21:17:48\n",
      "\n",
      "<__main__.main.<locals>.parser object at 0x281e268e0>\n",
      "./logs/T100_b64_adam_lr0.001\n",
      "epoch =8, train_loss = 0.0154, train_acc = 0.9248, test_loss = 0.0153, test_acc = 0.9251, max_test_acc = 0.9251\n",
      "train speed = 1743.0219 images/s, test speed = 1641.7587 images/s\n",
      "escape time = 2024-11-07 21:17:36\n",
      "\n",
      "<__main__.main.<locals>.parser object at 0x281e268e0>\n",
      "./logs/T100_b64_adam_lr0.001\n",
      "epoch =9, train_loss = 0.0153, train_acc = 0.9259, test_loss = 0.0153, test_acc = 0.9236, max_test_acc = 0.9251\n",
      "train speed = 1764.3341 images/s, test speed = 1710.2583 images/s\n",
      "escape time = 2024-11-07 21:16:35\n",
      "\n",
      "<__main__.main.<locals>.parser object at 0x281e268e0>\n",
      "./logs/T100_b64_adam_lr0.001\n",
      "epoch =10, train_loss = 0.0152, train_acc = 0.9260, test_loss = 0.0155, test_acc = 0.9244, max_test_acc = 0.9251\n",
      "train speed = 1777.8505 images/s, test speed = 1656.7757 images/s\n",
      "escape time = 2024-11-07 21:16:29\n",
      "\n",
      "<__main__.main.<locals>.parser object at 0x281e268e0>\n",
      "./logs/T100_b64_adam_lr0.001\n",
      "epoch =11, train_loss = 0.0151, train_acc = 0.9266, test_loss = 0.0152, test_acc = 0.9230, max_test_acc = 0.9251\n",
      "train speed = 1782.8836 images/s, test speed = 1655.2621 images/s\n",
      "escape time = 2024-11-07 21:16:20\n",
      "\n",
      "<__main__.main.<locals>.parser object at 0x281e268e0>\n",
      "./logs/T100_b64_adam_lr0.001\n",
      "epoch =12, train_loss = 0.0150, train_acc = 0.9269, test_loss = 0.0157, test_acc = 0.9223, max_test_acc = 0.9251\n",
      "train speed = 1804.5639 images/s, test speed = 1708.4879 images/s\n",
      "escape time = 2024-11-07 21:15:28\n",
      "\n",
      "<__main__.main.<locals>.parser object at 0x281e268e0>\n",
      "./logs/T100_b64_adam_lr0.001\n",
      "epoch =13, train_loss = 0.0149, train_acc = 0.9277, test_loss = 0.0152, test_acc = 0.9255, max_test_acc = 0.9255\n",
      "train speed = 1736.7863 images/s, test speed = 1583.6086 images/s\n",
      "escape time = 2024-11-07 21:18:03\n",
      "\n",
      "<__main__.main.<locals>.parser object at 0x281e268e0>\n",
      "./logs/T100_b64_adam_lr0.001\n",
      "epoch =14, train_loss = 0.0148, train_acc = 0.9284, test_loss = 0.0152, test_acc = 0.9238, max_test_acc = 0.9255\n",
      "train speed = 1753.7302 images/s, test speed = 1661.0310 images/s\n",
      "escape time = 2024-11-07 21:17:08\n",
      "\n",
      "<__main__.main.<locals>.parser object at 0x281e268e0>\n",
      "./logs/T100_b64_adam_lr0.001\n",
      "epoch =15, train_loss = 0.0147, train_acc = 0.9277, test_loss = 0.0150, test_acc = 0.9255, max_test_acc = 0.9255\n",
      "train speed = 1761.5335 images/s, test speed = 1540.0108 images/s\n",
      "escape time = 2024-11-07 21:17:36\n",
      "\n",
      "<__main__.main.<locals>.parser object at 0x281e268e0>\n",
      "./logs/T100_b64_adam_lr0.001\n",
      "epoch =16, train_loss = 0.0147, train_acc = 0.9281, test_loss = 0.0152, test_acc = 0.9271, max_test_acc = 0.9271\n",
      "train speed = 1739.2588 images/s, test speed = 1569.4352 images/s\n",
      "escape time = 2024-11-07 21:18:02\n",
      "\n",
      "<__main__.main.<locals>.parser object at 0x281e268e0>\n",
      "./logs/T100_b64_adam_lr0.001\n",
      "epoch =17, train_loss = 0.0146, train_acc = 0.9285, test_loss = 0.0149, test_acc = 0.9274, max_test_acc = 0.9274\n",
      "train speed = 1702.8164 images/s, test speed = 1595.0031 images/s\n",
      "escape time = 2024-11-07 21:18:56\n",
      "\n",
      "<__main__.main.<locals>.parser object at 0x281e268e0>\n",
      "./logs/T100_b64_adam_lr0.001\n",
      "epoch =18, train_loss = 0.0145, train_acc = 0.9292, test_loss = 0.0151, test_acc = 0.9233, max_test_acc = 0.9274\n",
      "train speed = 1472.2894 images/s, test speed = 1571.0694 images/s\n",
      "escape time = 2024-11-07 21:26:41\n",
      "\n",
      "<__main__.main.<locals>.parser object at 0x281e268e0>\n",
      "./logs/T100_b64_adam_lr0.001\n",
      "epoch =19, train_loss = 0.0145, train_acc = 0.9295, test_loss = 0.0150, test_acc = 0.9247, max_test_acc = 0.9274\n",
      "train speed = 1697.4926 images/s, test speed = 1559.3987 images/s\n",
      "escape time = 2024-11-07 21:19:22\n",
      "\n",
      "<__main__.main.<locals>.parser object at 0x281e268e0>\n",
      "./logs/T100_b64_adam_lr0.001\n",
      "epoch =20, train_loss = 0.0145, train_acc = 0.9298, test_loss = 0.0148, test_acc = 0.9261, max_test_acc = 0.9274\n",
      "train speed = 1710.9237 images/s, test speed = 1624.1380 images/s\n",
      "escape time = 2024-11-07 21:18:39\n",
      "\n",
      "<__main__.main.<locals>.parser object at 0x281e268e0>\n",
      "./logs/T100_b64_adam_lr0.001\n",
      "epoch =21, train_loss = 0.0144, train_acc = 0.9295, test_loss = 0.0148, test_acc = 0.9276, max_test_acc = 0.9276\n",
      "train speed = 1742.0450 images/s, test speed = 1605.3802 images/s\n",
      "escape time = 2024-11-07 21:17:55\n",
      "\n",
      "<__main__.main.<locals>.parser object at 0x281e268e0>\n",
      "./logs/T100_b64_adam_lr0.001\n",
      "epoch =22, train_loss = 0.0144, train_acc = 0.9299, test_loss = 0.0147, test_acc = 0.9269, max_test_acc = 0.9276\n",
      "train speed = 1644.9427 images/s, test speed = 1563.3516 images/s\n",
      "escape time = 2024-11-07 21:20:48\n",
      "\n",
      "<__main__.main.<locals>.parser object at 0x281e268e0>\n",
      "./logs/T100_b64_adam_lr0.001\n",
      "epoch =23, train_loss = 0.0143, train_acc = 0.9298, test_loss = 0.0148, test_acc = 0.9268, max_test_acc = 0.9276\n",
      "train speed = 1770.3257 images/s, test speed = 1621.1720 images/s\n",
      "escape time = 2024-11-07 21:17:09\n",
      "\n",
      "<__main__.main.<locals>.parser object at 0x281e268e0>\n",
      "./logs/T100_b64_adam_lr0.001\n",
      "epoch =24, train_loss = 0.0143, train_acc = 0.9297, test_loss = 0.0147, test_acc = 0.9274, max_test_acc = 0.9276\n",
      "train speed = 1789.1073 images/s, test speed = 1607.5321 images/s\n",
      "escape time = 2024-11-07 21:16:46\n",
      "\n",
      "<__main__.main.<locals>.parser object at 0x281e268e0>\n",
      "./logs/T100_b64_adam_lr0.001\n",
      "epoch =25, train_loss = 0.0143, train_acc = 0.9299, test_loss = 0.0147, test_acc = 0.9291, max_test_acc = 0.9291\n",
      "train speed = 1780.7566 images/s, test speed = 1675.9857 images/s\n",
      "escape time = 2024-11-07 21:16:38\n",
      "\n",
      "<__main__.main.<locals>.parser object at 0x281e268e0>\n",
      "./logs/T100_b64_adam_lr0.001\n",
      "epoch =26, train_loss = 0.0142, train_acc = 0.9305, test_loss = 0.0148, test_acc = 0.9259, max_test_acc = 0.9291\n",
      "train speed = 1763.2679 images/s, test speed = 1604.1843 images/s\n",
      "escape time = 2024-11-07 21:17:23\n",
      "\n",
      "<__main__.main.<locals>.parser object at 0x281e268e0>\n",
      "./logs/T100_b64_adam_lr0.001\n",
      "epoch =27, train_loss = 0.0142, train_acc = 0.9305, test_loss = 0.0148, test_acc = 0.9257, max_test_acc = 0.9291\n",
      "train speed = 1771.9904 images/s, test speed = 1663.0642 images/s\n",
      "escape time = 2024-11-07 21:16:55\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.main.<locals>.parser object at 0x281e268e0>\n",
      "./logs/T100_b64_adam_lr0.001\n",
      "epoch =28, train_loss = 0.0142, train_acc = 0.9312, test_loss = 0.0147, test_acc = 0.9283, max_test_acc = 0.9291\n",
      "train speed = 1769.3019 images/s, test speed = 1652.7254 images/s\n",
      "escape time = 2024-11-07 21:17:01\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import argparse\n",
    "import sys\n",
    "import datetime\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "from torch.cuda import amp\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torchvision\n",
    "import numpy as np\n",
    "\n",
    "from spikingjelly.activation_based import neuron, encoding, functional, surrogate, layer\n",
    "\n",
    "#定义SNN网络结构\n",
    "class SNN(nn.Module):\n",
    "    def __init__(self, tau):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layer = nn.Sequential(\n",
    "            layer.Flatten(),\n",
    "            #SNN全连接层\n",
    "            layer.Linear(28 * 28, 10, bias=False),\n",
    "            #SNN LIF Node\n",
    "            neuron.LIFNode(tau=tau, surrogate_function=surrogate.ATan()),\n",
    "            )\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return self.layer(x)\n",
    "\n",
    "def main():\n",
    "    '''\n",
    "    :return: None\n",
    "\n",
    "    * :ref:`API in English <lif_fc_mnist.main-en>`\n",
    "\n",
    "    .. _lif_fc_mnist.main-cn:\n",
    "\n",
    "    使用全连接-LIF的网络结构，进行MNIST识别。\\n\n",
    "    这个函数会初始化网络进行训练，并显示训练过程中在测试集的正确率。\n",
    "\n",
    "    * :ref:`中文API <lif_fc_mnist.main-cn>`\n",
    "\n",
    "    .. _lif_fc_mnist.main-en:\n",
    "\n",
    "    The network with FC-LIF structure for classifying MNIST.\\n\n",
    "    This function initials the network, starts trainingand shows accuracy on test dataset.\n",
    "    '''\n",
    "    class parser:\n",
    "        def __init__(self):\n",
    "            self.T = 100\n",
    "            self.device='cpu'\n",
    "            self.epochs = 100\n",
    "            self.b = 64\n",
    "            self.j=4\n",
    "            self.data_dir='./mnist_data'\n",
    "            self.out_dir='./logs'\n",
    "            self.resume = None\n",
    "            self.amp = False\n",
    "            self.opt = 'adam'\n",
    "            self.lr=1e-3\n",
    "            self.tau=2.0\n",
    "    '''        \n",
    "    parser = argparse.ArgumentParser(description='LIF MNIST Training')\n",
    "    parser.add_argument('-T', default=100, type=int, help='simulating time-steps')\n",
    "    parser.add_argument('-device', default='cuda:0', help='device')\n",
    "    parser.add_argument('-b', default=64, type=int, help='batch size')\n",
    "    parser.add_argument('-epochs', default=100, type=int, metavar='N',\n",
    "                        help='number of total epochs to run')\n",
    "    parser.add_argument('-j', default=4, type=int, metavar='N',\n",
    "                        help='number of data loading workers (default: 4)')\n",
    "    parser.add_argument('-data-dir', type=str, help='root dir of MNIST dataset')\n",
    "    parser.add_argument('-out-dir', type=str, default='./logs', help='root dir for saving logs and checkpoint')\n",
    "    parser.add_argument('-resume', type=str, help='resume from the checkpoint path')\n",
    "    parser.add_argument('-amp', action='store_true', help='automatic mixed precision training')\n",
    "    parser.add_argument('-opt', type=str, choices=['sgd', 'adam'], default='adam', help='use which optimizer. SGD or Adam')\n",
    "    parser.add_argument('-momentum', default=0.9, type=float, help='momentum for SGD')\n",
    "    parser.add_argument('-lr', default=1e-3, type=float, help='learning rate')\n",
    "    parser.add_argument('-tau', default=2.0, type=float, help='parameter tau of LIF neuron')\n",
    "    '''\n",
    "    args = parser()\n",
    "    #args = parser.parse_args()\n",
    "    #print(args)\n",
    "\n",
    "    net = SNN(tau=args.tau)\n",
    "\n",
    "    print(net)\n",
    "\n",
    "    net.to(args.device)\n",
    "\n",
    "    # 初始化数据加载器\n",
    "    train_dataset = torchvision.datasets.MNIST(\n",
    "        root=args.data_dir,\n",
    "        train=True,\n",
    "        transform=torchvision.transforms.ToTensor(),\n",
    "        download=True\n",
    "    )\n",
    "    test_dataset = torchvision.datasets.MNIST(\n",
    "        root=args.data_dir,\n",
    "        train=False,\n",
    "        transform=torchvision.transforms.ToTensor(),\n",
    "        download=True\n",
    "    )\n",
    "\n",
    "    train_data_loader = data.DataLoader(\n",
    "        dataset=train_dataset,\n",
    "        batch_size=args.b,\n",
    "        shuffle=True,\n",
    "        drop_last=True,\n",
    "        num_workers=args.j,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    test_data_loader = data.DataLoader(\n",
    "        dataset=test_dataset,\n",
    "        batch_size=args.b,\n",
    "        shuffle=False,\n",
    "        drop_last=False,\n",
    "        num_workers=args.j,\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "    scaler = None\n",
    "    if args.amp:\n",
    "        scaler = amp.GradScaler()\n",
    "\n",
    "    start_epoch = 0\n",
    "    max_test_acc = -1\n",
    "\n",
    "    optimizer = None\n",
    "    if args.opt == 'sgd':\n",
    "        optimizer = torch.optim.SGD(net.parameters(), lr=args.lr, momentum=args.momentum)\n",
    "    elif args.opt == 'adam':\n",
    "        optimizer = torch.optim.Adam(net.parameters(), lr=args.lr)\n",
    "    else:\n",
    "        raise NotImplementedError(args.opt)\n",
    "\n",
    "    if args.resume:\n",
    "        checkpoint = torch.load(args.resume, map_location='cpu')\n",
    "        net.load_state_dict(checkpoint['net'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        start_epoch = checkpoint['epoch'] + 1\n",
    "        max_test_acc = checkpoint['max_test_acc']\n",
    "    \n",
    "    out_dir = os.path.join(args.out_dir, f'T{args.T}_b{args.b}_{args.opt}_lr{args.lr}')\n",
    "\n",
    "    if args.amp:\n",
    "        out_dir += '_amp'\n",
    "\n",
    "    if not os.path.exists(out_dir):\n",
    "        os.makedirs(out_dir)\n",
    "        print(f'Mkdir {out_dir}.')\n",
    "\n",
    "    with open(os.path.join(out_dir, 'args.txt'), 'w', encoding='utf-8') as args_txt:\n",
    "        args_txt.write(str(args))\n",
    "\n",
    "    writer = SummaryWriter(out_dir, purge_step=start_epoch)\n",
    "    with open(os.path.join(out_dir, 'args.txt'), 'w', encoding='utf-8') as args_txt:\n",
    "        args_txt.write(str(args))\n",
    "        args_txt.write('\\n')\n",
    "        args_txt.write(' '.join(sys.argv))\n",
    "\n",
    "    #使用Poisson编码器对输入进行编码\n",
    "    encoder = encoding.PoissonEncoder()\n",
    "\n",
    "    for epoch in range(start_epoch, args.epochs):\n",
    "        start_time = time.time()\n",
    "        net.train()\n",
    "        train_loss = 0\n",
    "        train_acc = 0\n",
    "        train_samples = 0\n",
    "        for img, label in train_data_loader:\n",
    "            optimizer.zero_grad()\n",
    "            img = img.to(args.device)\n",
    "            label = label.to(args.device)\n",
    "            label_onehot = F.one_hot(label, 10).float() #对输出进行one-hot编码\n",
    "\n",
    "            if scaler is not None:\n",
    "                with amp.autocast(): #使用amp混合精度（Cuda)计算可以加速\n",
    "                    out_fr = 0. \n",
    "                    for t in range(args.T): #SNN内部T步时间步长\n",
    "                        encoded_img = encoder(img)\n",
    "                        out_fr += net(encoded_img)  #累加输出的脉冲个数\n",
    "                    out_fr = out_fr / args.T #输出脉冲个数归一化\n",
    "                    loss = F.mse_loss(out_fr, label_onehot) #最小化SNN预测值和one-hot编码的区别\n",
    "                scaler.scale(loss).backward() #使用混合精度计算计算loss\n",
    "                scaler.step(optimizer)        #使用混合精度计算进行optimization\n",
    "                scaler.update()\n",
    "            else:   #同上\n",
    "                out_fr = 0. \n",
    "                for t in range(args.T):\n",
    "                    encoded_img = encoder(img)\n",
    "                    out_fr += net(encoded_img)\n",
    "                out_fr = out_fr / args.T\n",
    "                loss = F.mse_loss(out_fr, label_onehot)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            train_samples += label.numel()\n",
    "            train_loss += loss.item() * label.numel()\n",
    "            train_acc += (out_fr.argmax(1) == label).float().sum().item()  #计算训练精确度\n",
    "\n",
    "            functional.reset_net(net) #注意：此步为必要步骤，让LIF node的membrane potential回到0\n",
    "\n",
    "        train_time = time.time()\n",
    "        train_speed = train_samples / (train_time - start_time)\n",
    "        train_loss /= train_samples\n",
    "        train_acc /= train_samples\n",
    "\n",
    "        writer.add_scalar('train_loss', train_loss, epoch)\n",
    "        writer.add_scalar('train_acc', train_acc, epoch)\n",
    "\n",
    "        net.eval()\n",
    "        test_loss = 0\n",
    "        test_acc = 0\n",
    "        test_samples = 0\n",
    "        with torch.no_grad():\n",
    "            for img, label in test_data_loader:  #类似训练步骤计算测试数据集上的性能\n",
    "                img = img.to(args.device)\n",
    "                label = label.to(args.device)\n",
    "                label_onehot = F.one_hot(label, 10).float()\n",
    "                out_fr = 0.\n",
    "                for t in range(args.T):\n",
    "                    encoded_img = encoder(img)\n",
    "                    out_fr += net(encoded_img)\n",
    "                out_fr = out_fr / args.T\n",
    "                loss = F.mse_loss(out_fr, label_onehot)\n",
    "\n",
    "                test_samples += label.numel()\n",
    "                test_loss += loss.item() * label.numel()\n",
    "                test_acc += (out_fr.argmax(1) == label).float().sum().item()\n",
    "                functional.reset_net(net)\n",
    "        test_time = time.time()\n",
    "        test_speed = test_samples / (test_time - train_time)\n",
    "        test_loss /= test_samples\n",
    "        test_acc /= test_samples\n",
    "        writer.add_scalar('test_loss', test_loss, epoch)\n",
    "        writer.add_scalar('test_acc', test_acc, epoch)\n",
    "\n",
    "        save_max = False\n",
    "        if test_acc > max_test_acc:\n",
    "            max_test_acc = test_acc\n",
    "            save_max = True\n",
    "\n",
    "        checkpoint = {\n",
    "            'net': net.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'epoch': epoch,\n",
    "            'max_test_acc': max_test_acc\n",
    "        }\n",
    "\n",
    "        if save_max:\n",
    "            torch.save(checkpoint, os.path.join(out_dir, 'checkpoint_max.pth'))\n",
    "\n",
    "        torch.save(checkpoint, os.path.join(out_dir, 'checkpoint_latest.pth'))\n",
    "\n",
    "        print(args)\n",
    "        print(out_dir)\n",
    "        print(f'epoch ={epoch}, train_loss ={train_loss: .4f}, train_acc ={train_acc: .4f}, test_loss ={test_loss: .4f}, test_acc ={test_acc: .4f}, max_test_acc ={max_test_acc: .4f}')\n",
    "        print(f'train speed ={train_speed: .4f} images/s, test speed ={test_speed: .4f} images/s')\n",
    "        print(f'escape time = {(datetime.datetime.now() + datetime.timedelta(seconds=(time.time() - start_time) * (args.epochs - epoch))).strftime(\"%Y-%m-%d %H:%M:%S\")}\\n')\n",
    "\n",
    "    # 保存绘图用数据\n",
    "    net.eval()\n",
    "    # 注册钩子\n",
    "    output_layer = net.layer[-1] # 输出层\n",
    "    output_layer.v_seq = []\n",
    "    output_layer.s_seq = []\n",
    "    def save_hook(m, x, y):\n",
    "        m.v_seq.append(m.v.unsqueeze(0))\n",
    "        m.s_seq.append(y.unsqueeze(0))\n",
    "\n",
    "    output_layer.register_forward_hook(save_hook)\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        img, label = test_dataset[0]\n",
    "        img = img.to(args.device)\n",
    "        out_fr = 0.\n",
    "        for t in range(args.T):\n",
    "            encoded_img = encoder(img)\n",
    "            out_fr += net(encoded_img)\n",
    "        out_spikes_counter_frequency = (out_fr / args.T).cpu().numpy()\n",
    "        print(f'Firing rate: {out_spikes_counter_frequency}')\n",
    "\n",
    "        output_layer.v_seq = torch.cat(output_layer.v_seq)\n",
    "        output_layer.s_seq = torch.cat(output_layer.s_seq)\n",
    "        v_t_array = output_layer.v_seq.cpu().numpy().squeeze()  # v_t_array[i][j]表示神经元i在j时刻的电压值\n",
    "        np.save(\"v_t_array.npy\",v_t_array)\n",
    "        s_t_array = output_layer.s_seq.cpu().numpy().squeeze()  # s_t_array[i][j]表示神经元i在j时刻释放的脉冲，为0或1\n",
    "        np.save(\"s_t_array.npy\",s_t_array)\n",
    "\n",
    "\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d56e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(v_t_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896e199d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
